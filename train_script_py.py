# -*- coding: utf-8 -*-
"""train_script.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ifZup-JoERVg3PCM_-UopV-qYqLjZsyv
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel
import random
import time
import matplotlib.pyplot as plt


import socket

def find_available_port():
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    sock.bind(('localhost', 0))  # Bind to any available port on localhost
    _, port = sock.getsockname()
    sock.close()
    return port

port = find_available_port()
print(f"Available Port: {port}")




# Number of trainers
num_trainers = 2

# File content with graph for train and test set
train_file = '/content/train.txt'
test_file = '/content/test.txt'  # Provide the path to your test file

# Reading file and parsing triples
with open(train_file, 'r') as file:
    lines = file.readlines()

# Triples parsed into subject, relation, and object
triples = [line.strip().split('\t') for line in lines]

# Extracting entities and relations
all_entities = set([triple[0] for triple in triples] + [triple[2] for triple in triples])
all_relations = set([triple[1] for triple in triples])

# Mapping entities and relations into identifiers
entity_to_id = {entity: idx for idx, entity in enumerate(all_entities)}
relation_to_id = {relation: idx for idx, relation in enumerate(all_relations)}

print(entity_to_id)
print(relation_to_id)

# Encoding triples into numerical identifiers (tensors)
train_data = {'subject': torch.tensor([entity_to_id[triple[0]] for triple in triples], dtype=torch.long),
              'relation': torch.tensor([relation_to_id[triple[1]] for triple in triples], dtype=torch.long),
              'object': torch.tensor([entity_to_id[triple[2]] for triple in triples], dtype=torch.long)}

# Reading test file and parsing test triples
with open(test_file, 'r') as file:
    test_lines = file.readlines()

test_triplets = [line.strip().split('\t') for line in test_lines]



# Encoding test triples into numerical identifiers (tensors)
test_data = {
    'subject': torch.tensor([entity_to_id.get(triple[0], -1) for triple in test_triplets], dtype=torch.long),
    'relation': torch.tensor([relation_to_id.get(triple[1], -1) for triple in test_triplets], dtype=torch.long),
    'object': torch.tensor([entity_to_id.get(triple[2], -1) for triple in test_triplets], dtype=torch.long)
}

# Filter out entities with default value (-1)
test_data = {key: value[value != -1] for key, value in test_data.items()}


# Step 2: Partitioning (Vertex-Cut)
num_partitions = 2

train_partitioned_data_lists = [{key: data[i::num_partitions] for key, data in train_data.items()} for i in range(num_partitions)]

# Step 3: Neighborhood Expansion
def neighborhood_expansion(partition, num_hops=1):
    expanded_partition = set(partition['subject'].tolist())
    for _ in range(num_hops):
        for edge_idx in range(len(partition['subject'])):
            expanded_partition.add(partition['object'][edge_idx].item())
    return list(expanded_partition)

expanded_partitions = [neighborhood_expansion(partition) for partition in train_partitioned_data_lists]

# Step 4: Negative Sampling
def negative_sampling(partition, num_negative_samples):
    try:
        negative_samples_list = []
        for _ in range(num_negative_samples):
            negative_samples = {
                'subject': partition['subject'][random.randint(0, len(partition['subject']) - 1)].unsqueeze(0),
                'relation': partition['relation'][random.randint(0, len(partition['relation']) - 1)].unsqueeze(0),
                'object': partition['object'][random.randint(0, len(partition['object']) - 1)].unsqueeze(0),
            }
            negative_samples_list.append(negative_samples)

        print("Negative Samples List:", negative_samples_list)
        return negative_samples_list
    except Exception as e:
        print(f"Error in negative_sampling: {e}")
        print("Partition:", partition)
        print("Num Negative Samples:", num_negative_samples)
        raise

num_negative_samples = 1
negative_samples_list = [negative_sampling(partition, num_negative_samples) for partition in train_partitioned_data_lists]

# Step 5: Edge_minibatching
def edge_minibatching(partition, batch_size):
    indices = list(range(len(partition)))
    random.shuffle(indices)

    # Sort the indices
    indices.sort()

    num_batches = len(partition) // batch_size
    remainder = len(partition) % batch_size

    batches = [
        {
            'subject': partition['subject'][indices[j * batch_size:(j + 1) * batch_size]].long(),
            'relation': partition['relation'][indices[j * batch_size:(j + 1) * batch_size]].long(),
            'object': partition['object'][indices[j * batch_size:(j + 1) * batch_size]].long()
        }
        for j in range(num_batches)
    ]

    # Handling remaining samples into batches
    if remainder > 0:
        last_batch_start = num_batches * batch_size
        last_batch = {
            'subject': partition['subject'][indices[last_batch_start:]].long(),
            'relation': partition['relation'][indices[last_batch_start:]].long(),
            'object': partition['object'][indices[last_batch_start:]].long()
        }
        batches.append(last_batch)

    print("Batches:", batches)
    return batches

batch_size = len(train_data['object'])
batches = edge_minibatching(train_partitioned_data_lists[0], batch_size)

# Step 6: Graph embedding model and optimization
class GraphEmbeddingModel(nn.Module):
    def __init__(self, num_entities, num_relations, embedding_size):
        super(GraphEmbeddingModel, self).__init__()
        self.embedding_size = embedding_size
        self.entity_embedding = nn.Embedding(num_entities, embedding_size)
        self.relation_embedding = nn.Embedding(num_relations, embedding_size)

    def forward(self, data):
        subject_embedding = self.entity_embedding(data['subject'])
        relation_embedding = self.relation_embedding(data['relation'])
        object_embedding = self.entity_embedding(data['object'])
        score = torch.sum(subject_embedding * relation_embedding, dim=1)
        return score

# Hyperparameters
num_entities = len(all_entities)
num_relations = len(all_relations)
embedding_size = 75
num_epochs = 10
learning_rate = 0.01
batch_size = len(train_data['object'])

# Model, Loss, and Optimizer
model = GraphEmbeddingModel(num_entities, num_relations, embedding_size)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Set up port server for using trainers
def init_process(rank, world_size):
    dist.init_process_group(
        backend='gloo',
        init_method='tcp://127.0.0.1:35765',
        rank=rank,
        world_size=world_size
    )

def cleanup():
    dist.destroy_process_group()

# Intitialize and Synchronize trainers
if dist.is_initialized():
    print("Process group is already initialized.")
else:
    for rank in range(num_trainers):
        init_process(rank, num_trainers)






# Wrapping model using DistributedDataParallel
model = DistributedDataParallel(model)

def evaluate_triplets(model, test_data, entity_to_id, relation_to_id, k=10):
    model.eval()
    hits_at_k = 0
    reciprocal_ranks = 0

    for i in range(len(test_data['subject'])):
        try:
            head_id, relation_id, tail_id = (
                test_data['subject'][i].item(),
                test_data['relation'][i].item(),
                test_data['object'][i].item()
            )

            # Forward pass
            triplet_data = {
                'subject': torch.tensor([head_id], dtype=torch.long),
                'relation': torch.tensor([relation_id], dtype=torch.long),
                'object': torch.tensor([tail_id], dtype=torch.long)
            }

            with torch.no_grad():
                scores = model(triplet_data)

            # Rank of the correct triplets
            rank = 1 + torch.sum(scores > scores[0]).item()

            # Update metrics
            reciprocal_ranks += 1.0 / rank
            if rank <= k:
                hits_at_k += 1

        except KeyError as e:
            print(f"Error processing triplet {test_data['subject'][i].item()}, {test_data['relation'][i].item()}, {test_data['object'][i].item()}: {e}")

    # Calculate metrics
    num_valid_triplets = len(test_data['subject'])
    mean_reciprocal_rank = reciprocal_ranks / num_valid_triplets
    hits_at_k /= num_valid_triplets

    return mean_reciprocal_rank, hits_at_k






# Training Loop
for epoch in range(num_epochs):
    start_time = time.time()
    total_loss = 0.0
    for partition, negative_samples in zip(train_partitioned_data_lists, negative_samples_list):
        indices = list(range(len(partition)))
        random.shuffle(indices)

        # Sorting indices
        indices.sort()

        num_batches = max(1, len(partition) // batch_size)

        for i in range(num_batches):
            batch_data = None
            try:
                batch_start = i * batch_size
                batch_end = min((i + 1) * batch_size, len(partition))

                batch_data = {
                    'subject': partition['subject'][indices[batch_start:batch_end]].long(),
                    'relation': partition['relation'][indices[batch_start:batch_end]].long(),
                    'object': partition['object'][indices[batch_start:batch_end]].long(),
                }

                positive_score = model(batch_data)

                # Print negative_samples for debugging
                print("Negative Samples List:", negative_samples)

                negative_data = {
                    key: torch.cat([sample[key] for sample in negative_samples], dim=0) for key in ['subject', 'relation', 'object']
                }

                negative_scores = model(negative_data)

                positive_labels = torch.ones_like(positive_score)
                negative_labels = torch.zeros_like(negative_scores)
                loss = criterion(positive_score, positive_labels) + criterion(negative_scores, negative_labels)

                total_loss += loss.item()

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            except Exception as e:
                print(f"Error in epoch {epoch + 1}, batch {i + 1}: {e}")
                print("Batch Data:", batch_data)
                print("Partition:", partition)
                print("Indices:", indices)
                print("Batch Start:", batch_start)
                print("Batch End:", batch_end)
                raise

        epoch_time = time.time() - start_time
        print(f"Epoch {epoch + 1}/{num_epochs}, Partition Size: {num_partitions}, Epoch Time: {epoch_time:.2f} seconds")

    average_loss = total_loss / max(1, (num_batches * len(train_partitioned_data_lists)))
    print(f"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}")

    # Evaluation on test set
    evaluation_interval = 1
    if (epoch + 1) % evaluation_interval == 0:
        test_mrr, test_hits_at_k = evaluate_triplets(model, test_data, entity_to_id, relation_to_id, k=10)
        print(f"Epoch {epoch + 1}/{num_epochs}, Test MRR: {test_mrr}, Test Hits@10: {test_hits_at_k}")


# Analyze and report partition statistics
partition_sizes = [len(partition) for partition in expanded_partitions]
average_partition_size = sum(partition_sizes) / len(partition_sizes)
std_dev_partition_size = (sum((size - average_partition_size) ** 2 for size in partition_sizes) / len(partition_sizes)) ** 0.5
max_partition_size = max(partition_sizes)
min_partition_size = min(partition_sizes)
balance_ratio = max_partition_size / min_partition_size


# Compute number of nodes per partition
num_nodes_per_partition = [
    len(set(partition['subject'].tolist() + partition['object'].tolist()))
    for partition in train_partitioned_data_lists
]

# Display num_nodes_per_partition
print("Number of Nodes per Partition:", num_nodes_per_partition)

# Report partition statistics
print(f"Partition sizes: {partition_sizes}")
print(f"Average Partition Size: {average_partition_size}")
print(f"Standard Deviation of Partition Size: {std_dev_partition_size}")
print(f"Max Partition Size: {max_partition_size}")
print(f"Min Partition Size: {min_partition_size}")
print(f"Balance Ratio (Max/Min): {balance_ratio}")



# Epochs
epochs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
#partition size = 8
epoch_time_8 = [0.17, 0.15, 0.15, 0.16, 0.16, 0.19, 0.16, 0.16, 0.16, 0.15]
#partition size 4
epoch_time_4 = [0.10, 0.08, 0.08, 0.07, 0.09, 0.08, 0.09, 0.10, 0.10, 0.11]
#partition size 2
epoch_time_2 = [0.05, 0.04, 0.04, 0.04, 0.04, 0.05, 0.05, 0.05, 0.06, 0.06]
average_loss_8 = [7.676589280366898, 3.5982721596956253, 1.3256578519940376, 0.39091868477407843, 0.06316455273190513, 0.014943347981898114, 0.008350121701369062, 0.006498440139694139, 0.0057220370072172955, 0.005304279547999613]

average_loss_4 = [10.00130759930471, 6.740513608063338, 4.451504339563144, 2.9473312198861095, 1.7140315721371735, 0.82366463665943, 0.3401643522072959, 0.09040413292368044, 0.020904243457607663, 0.007688964212235305]

average_loss_2 = [2.017099976539612, 0.8991904854774475, 0.30594784021377563, 0.08871296048164368, 0.028541725128889084, 0.011058892123401165, 0.0050853469292633235, 0.002693038433790207, 0.0015986234648153186, 0.0010407158988527954]



if __name__ == "__main__":


# Plotting
    plt.figure(figsize=(10, 6))

# Subplot 1: Epoch Time
    plt.subplot(2, 1, 1)
    plt.plot(epochs, epoch_time_8, marker='o', linestyle='-', color='b', label='Partition Size: 8')
    plt.plot(epochs, epoch_time_4, marker='o', linestyle='-', color='g', label='Partition Size: 4')
    plt.plot(epochs, epoch_time_2, marker='o', linestyle='-', color='r', label='Partition Size: 2')
    plt.title('Epoch Time Over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Epoch Time (seconds)')
    plt.legend()

# Subplot 2: Average Loss
    plt.subplot(2, 1, 2)
    plt.plot(epochs, average_loss_8, marker='o', linestyle='-', color='b', label= 'Partition Size: 8')
    plt.plot(epochs, average_loss_4, marker='o', linestyle='-', color='r', label= 'Partition Size: 4')
    plt.plot(epochs, average_loss_2, marker='o', linestyle='-', color='g', label = 'Partition Size: 2')
    plt.title('Average Loss Over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Average Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

